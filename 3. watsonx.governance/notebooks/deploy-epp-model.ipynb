{"cells": [{"cell_type": "markdown", "id": "71fc2c8c-188b-42ef-b898-93a987f2c4e7", "metadata": {}, "source": "# Deploy a employee promotion prediction model to SAP AI Core"}, {"cell_type": "markdown", "id": "f5599f85-8647-4a00-a339-d11966a5377d", "metadata": {}, "source": "In this notebook we deploy the model to [SAP AI Core](https://help.sap.com/docs/sap-ai-core) as a custom ML engine IBM Watson OpenScale. We will go through the steps to authenticate to SAP AI Core, add secrets for docker registry, GitHub, and IBM Cloud credentials, onboard a github repository for a serving executable, create an Application and Configuration for the deployment, and deploy the serving executable on SAP AI Core. Once the serving runtime is succesfully deployed, we test its endpoints using sample data.\n\nYou will learn:\n\n- How to connect to SAP AI Core with Python SDK\n- How to create a workflow for a serving runtime\n- How to deploy a custom ML engine and make REST calls to it's endpoints\n\n### Prerequisites\n\n- [SAP Businness Technology Platform (BTP)](https://help.sap.com/docs/btp) services and applications\n    - [SAP AI Core](https://discovery-center.cloud.sap/serviceCatalog/sap-ai-core?region=all)\n    - [SAP AI Launchpad](https://discovery-center.cloud.sap/serviceCatalog/sap-ai-launchpad?region=all)\n- Access to a Jupyter Notebook environment, such as a [project](https://www.ibm.com/docs/en/watsonx?topic=projects) in watsonx\n- A personal access token for your Docker registry\n- A [GitHub repository](https://docs.github.com/en/get-started/quickstart/create-a-repo) and [personal access token (PAT)](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens) to access the repository\n\n## Table of Contents\n\n1.  [Step 1: Set up SAP AI Core SDK to connect with SAP AI Core](#aicore_sdk)\n\n1.  [Step 2: Onboard GitHub to SAP AI Core](#onboard)\n\n1.  [Step 3: Create a generic secret for connecting to OpenScale](#generic_secret)\n\n1.  [Step 4: Create a Docker registry secret for Docker Registry](#docker_registry)\n\n1.  [Step 5: Create an Application](#create_application)\n\n1.  [Step 6: Create a deployment configuration](#create_configuration)\n\n1.  [Step 7: Start the deployment](#start_deployment)\n\n1.  [Step 8: Make a prediction](#make_prediction)\n\n1.  [Step 9: Make a prediction and send logging to OpenScale](#predict_and_log)\n\n1.  [Step 10: Stop the deployment](#stop_deployment)\n\n1.  [Summary](#summary)"}, {"cell_type": "markdown", "id": "0caa8233", "metadata": {}, "source": "Before you begin, you'll need to use your own credentials to replace what's in the examples below. To avoid sharing them accidentally, copy the following code block into a new code cell, and make it a hidden cell by adding `# @hidden_cell` to the top.\n\n```\n# Set your SAP AI Core credential\nclient_id = '<your_client_id>'\nclient_secret = '<your_client_secret>'\nbase_url = '<ai_api_url>' + '/v2'\nauth_url = '<url>' + '/oauth/token'\n\n# IBM Cloud API Key and OpenScale instance details to be stored in an AI Core secret\ncloudapikey = '<your_ibm_cloud_api_key>'\nservice_instance_id = '<openscale_service_instance_id>'\nsubscription_id = '<openscale_subscription_id>'\nservice_url = '<openscale_service_url>'\nsecret_name = \"<name_of_generic_secret>\"\n\n# Set the GitHub credential\ngithub_cred_name = '<any_unique_name_for_the_git_repository_credential>'\ngithub_repo_url = 'https://github.com/<your_username>/<repo_name>'\ngithub_username = '<your_username>'\ngithub_password = '<your_personal_assess_token>'\n\n# Set the Docker Registry credential with your IBM Entitlement Key\nentitled_registry_credential = \"{\\\"auths\\\":{\\\"cp.icr.io\\\":{\\\"username\\\":\\\"cp\\\",\\\"password\\\":\\\"<your_key>\\\"}}}\"\n```"}, {"cell_type": "code", "execution_count": 1, "id": "a9dc08ec", "metadata": {}, "outputs": [], "source": "# The code was removed by Watson Studio for sharing."}, {"cell_type": "markdown", "id": "2dd6fd6e", "metadata": {}, "source": "<a id=\"aicore_sdk\"></a>\n### Step 1: Set up SAP AI Core SDK to connect with SAP AI Core"}, {"cell_type": "markdown", "id": "d0d69a32", "metadata": {}, "source": "The SAP AI Core SDK is a Python-based SDK that lets you access SAP AI Core using Python methods and data structures. It provides tools that help you to manage your scenarios and workflows in SAP AI Core. The SAP AI Core SDK can be used to interact with SAP AI Core. It provides access to all public lifecycle and administration APIs.\n\nThe command line tool `pip` is the Python package installer. You can use `pip` to install the SAP AI Core SDK for Python:"}, {"cell_type": "code", "execution_count": 2, "id": "108849ba", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: ai-core-sdk in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (2.1.1)\nRequirement already satisfied: ai-api-client-sdk==2.0.4 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ai-core-sdk) (2.0.4)\nRequirement already satisfied: click~=8.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ai-core-sdk) (8.1.7)\nRequirement already satisfied: aenum~=3.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ai-api-client-sdk==2.0.4->ai-core-sdk) (3.1.15)\nRequirement already satisfied: pyhumps~=3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ai-api-client-sdk==2.0.4->ai-core-sdk) (3.8.0)\nRequirement already satisfied: requests<3.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from ai-api-client-sdk==2.0.4->ai-core-sdk) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0->ai-api-client-sdk==2.0.4->ai-core-sdk) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0->ai-api-client-sdk==2.0.4->ai-core-sdk) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0->ai-api-client-sdk==2.0.4->ai-core-sdk) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from requests<3.0->ai-api-client-sdk==2.0.4->ai-core-sdk) (2023.11.17)\nNote: you may need to restart the kernel to use updated packages.\n"}], "source": "# Install SAP AI Core SDK\n%pip install ai-core-sdk"}, {"cell_type": "markdown", "id": "d8c40189", "metadata": {}, "source": "Connect with SAP AI Core with your AI Core credential:"}, {"cell_type": "code", "execution_count": 3, "id": "872f075e-1aac-490d-9897-9eb60a0b9f45", "metadata": {}, "outputs": [], "source": "# Load AI Core Client Library\nfrom ai_core_sdk.ai_core_v2_client import AICoreV2Client\n\n# Create a client connection\nai_core_client = AICoreV2Client(\n    base_url = base_url,\n    auth_url = auth_url,\n    client_id = client_id,\n    client_secret = client_secret\n)"}, {"cell_type": "markdown", "id": "2da24e1d", "metadata": {}, "source": "<a id=\"onboard\"></a>\n### Step 2: Onboard GitHub to SAP AI Core"}, {"cell_type": "markdown", "id": "bd143d0c", "metadata": {}, "source": "You can use your own git repository to version control your SAP AI Core templates. The GitOps onboarding to SAP AI Core instances involves setting up your git repository and synchronizing your content.\n\nOn-board a new GitHub repository:"}, {"cell_type": "code", "execution_count": 4, "id": "04decc27", "metadata": {}, "outputs": [{"data": {"text/plain": "<ai_core_sdk.models.base_models.Message at 0x7f8dbe7c7700>"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "ai_core_client.repositories.create(\n    name = github_cred_name,\n    url = github_repo_url,\n    username = github_username,\n    password = github_password\n)"}, {"cell_type": "markdown", "id": "fb72b35b", "metadata": {}, "source": "Execute the command below to check the on-boarding status:"}, {"cell_type": "code", "execution_count": 5, "id": "9273689c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Name: aicore-pipeline-openscale-demo\nURL: https://github.com/kevinxhuang/aicore-pipeline-openscale-demo\nStatus: RepositoryStatus.COMPLETED\n"}], "source": "response = ai_core_client.repositories.query()\n\nfor repository in response.resources:\n    if repository.name == 'aicore-pipeline-openscale-demo':\n        print('Name:', repository.name)\n        print('URL:', repository.url)\n        print('Status:', repository.status)"}, {"cell_type": "markdown", "id": "63436755-1878-4ef5-98b8-d0bacf184595", "metadata": {}, "source": "<a id=\"generic_secret\"></a>\n### Step 3: Create a generic secret for connecting to OpenScale"}, {"cell_type": "markdown", "id": "c11cc1ec-00c5-418e-9e13-c5eb48b1edbe", "metadata": {}, "source": "In the below cell, we store the IBM Cloud API Key and OpenScale service instance details in SAP AI Core as a generic secret. The secret will be passed to the deployed serviing runtime container as environmental variables.\n\nThe secret in SAP AI Core needs to be base64 encoded. The following code cell encodes data in base64 format."}, {"cell_type": "code", "execution_count": 6, "id": "8d7b2a91-32e2-4840-9b92-c499337e281f", "metadata": {}, "outputs": [], "source": "import json\nimport base64\n\n# Define the JSON object\njson_data = {\n              \"cloudapikey\": cloudapikey,\n              \"service_instance_id\": service_instance_id,\n              \"subscription_id\": subscription_id,\n              \"service_url\": service_url\n            }\n\n# Convert JSON to string\njson_string = json.dumps(json_data)\n\n# Encode the string in Base64\nbase64_encoded = base64.b64encode(json_string.encode('utf-8')).decode('utf-8')"}, {"cell_type": "markdown", "id": "be404307-277b-4d5c-9392-5ed0988b5b3b", "metadata": {}, "source": "The encoded data is then stored as a secret on SAP AI Core by sending a post request."}, {"cell_type": "code", "execution_count": 7, "id": "66d23f6b-b2fd-4fda-9075-41445ef419ec", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Secret created successfully\n"}], "source": "import requests\n\nurl = f\"{base_url}/admin/secrets\"\nheaders = {\n    \"Authorization\": ai_core_client.rest_client.get_token(),\n    \"Content-Type\": \"application/json\",\n    \"AI-Resource-Group\": \"default\"\n}\n\ndata = {\n    \"name\": secret_name,\n    \"data\": { \"credentials\": base64_encoded }\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n\n# Check the response\nif response.status_code == 200:\n    print(\"Secret created successfully\")\nelse:\n    print(f\"Error creating secret. Status code: {response.status_code}\")\n    print(response.text)"}, {"cell_type": "markdown", "id": "66544389-d1a9-4a3a-896f-1a9376c86503", "metadata": {}, "source": "<a id=\"docker_registry\"></a>\n### Step 4: Create a Docker registry secret"}, {"cell_type": "markdown", "id": "11052790", "metadata": {}, "source": "The runtime container image required to serve the custom-trained model is stored in a Docker registry. The credentials for Docker registries are managed using secrets.\n\nCreate a Docker registry secret for connecting SAP AI Core to your Docker registry:"}, {"cell_type": "code", "execution_count": 8, "id": "705cc593-5048-476d-afa4-86eeb61a8fe1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{'message': 'secret has been created'}\n"}], "source": "response = ai_core_client.docker_registry_secrets.create(\n    name = \"dockerhub-secret\",\n    data = {\n        \".dockerconfigjson\": entitled_registry_credential\n    }\n)\n\nprint(response.__dict__)"}, {"cell_type": "markdown", "id": "0a32cd0f-acfe-420f-8bf6-059208eca191", "metadata": {}, "source": "<a id=\"create_application\"></a>\n### Step 5: Create an Application"}, {"cell_type": "markdown", "id": "26812e69", "metadata": {}, "source": "SAP AI Core supports multiple hyperscaler object stores, such as Amazon S3, OSS (Alicloud Object Storage Service), SAP HANA Cloud, Data Lake and Azure Blob Storage. The connected storage stores your dataset, models and other cache files of the Metaflow Library for SAP AI Core. Storage credentials are managed using secrets. You can create multiple object store secrets.\n\nYou will create a folder in your GitHub repository connected SAP AI Core, where you will store the workflow (executable). You will then register this folder as an\u00a0**Application**\u00a0in SAP AI Core to enable syncing of the workflow as an executable.\n\nYou can create multiple\u00a0**Applications**\u00a0in SAP AI Core for syncing multiple folders. This helps you organize separate folders for storing workflows YAML files for separate use cases.\n\n1. Create a executable YAML file named\u00a0`openscale-demo/serving_executable.yaml`\u00a0in your GitHub repository connected to SAP AI Core.\n2. Edit and execute the code below to create an\u00a0**Application**\u00a0and sync the folder\u00a0`openscale-demo`\n3. Verify your workflow sync status, using the following code\n\nYou may use the existing GitHub path which is already tracked synced to your application of SAP AI Core.\n\n**IMPORTANT**:\u00a0The structure(schemas) of workflows and executables are different for both training and serving in SAP AI Core. For available options for the schemas you must refer to the\u00a0[official help guide of SAP AI Core](https://help.sap.com/docs/AI_CORE/2d6c5984063c40a59eda62f4a9135bee/8a1f91a18cf0473e8689789f1636675a.html?locale=en-US).\n\n```yaml\napiVersion: ai.sap.com/v1alpha1\nkind: ServingTemplate\nmetadata:\n  name: openscale-demo-240213\n  annotations:\n    scenarios.ai.sap.com/description: \"Custom ML Engine\"\n    scenarios.ai.sap.com/name: \"openscale-demo\"\n    executables.ai.sap.com/description: \"Custom ML Engine executable\"\n    executables.ai.sap.com/name: \"openscale-demo-exectuable\"\n  labels:\n    scenarios.ai.sap.com/id: \"openscale-demo\"\n    ai.sap.com/version: \"1.0.0\"\nspec:\n  inputs:\n    parameters:\n      - name: greetmessage # placeholder name\n        type: string\n  template:\n    apiVersion: \"serving.kserve.io/v1beta1\"\n    metadata:\n      labels: |\n        ai.sap.com/resourcePlan: starter # computing power\n    spec: |\n      predictor:\n        imagePullSecrets:\n          - name: dockerhub-secret\n        containers:\n        - name: kserve-container\n          image: \"kevinxhuang/demo:openscale\"\n          ports:\n            - containerPort: 7000 # customizable port\n              protocol: TCP\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - >\n              set -e && echo \"Starting\" && gunicorn --chdir /app/src auto:app -b 0.0.0.0:7000\n          env:\n            - name: OPENSCALE_CREDS\n              valueFrom:\n                secretKeyRef:\n                  name: openscale-secret # a generic secret name of your choice\n                  key: credentials\n            - name: greetingmessage # different name to avoid confusion\n              value: \"{{inputs.parameters.greetmessage}}\"\n\n```\n\n**Understanding your serving executable**\n\n- You use the\u00a0`starter`\u00a0computing resource plan with\u00a0`ai.sap.com/resourcePlan`. To start, using a non-GPU based resource plan for serving (like\u00a0`starter`) is cost effective. Find out more about available resource plans in\u00a0[the help portal](https://help.sap.com/docs/AI_CORE/2d6c5984063c40a59eda62f4a9135bee/57f4f19d9b3b46208ee1d72017d0eab6.html?locale=en-US).\n- You set the auto scaling of the server with the parameters:\u00a0`minReplicas`\u00a0and\u00a0`maxReplicas`.\n- You set the credentials to access the docker registry via\u00a0`imagePullSecrets`. You must ensure that if you are using a public docker registry that has the file type\u00a0`docker.io`, your secret points to the URL\u00a0`https://index.docker.io`. You may delete and recreate the docker registry secret. This will not affect training templates running in parallel.\n- You use the placeholder\u00a0`env`\u00a0to pass your\u00a0`inputs`\u00a0values as environment variables in your Docker image.\n- You must set the name of the container to `kfserving-container` or `kserve-container`."}, {"cell_type": "markdown", "id": "d5c517d5-8841-4eef-9f40-e40bb96ecdfd", "metadata": {}, "source": "Create an `Application` in SAP AI Core."}, {"cell_type": "code", "execution_count": 9, "id": "529038b0-d504-4933-b7da-c9487862362e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{'id': 'openscale-demo-app', 'message': 'Application has been successfully created.'}\n"}], "source": "response = ai_core_client.applications.create(\n    application_name = \"openscale-demo-app\",\n    revision = \"HEAD\",\n    repository_url = github_repo_url,\n    path = \"openscale-demo\"\n)\n\nprint(response.__dict__)"}, {"cell_type": "markdown", "id": "78f8d7c1", "metadata": {}, "source": "Verify your workflow sync status:"}, {"cell_type": "code", "execution_count": 10, "id": "f897b944-c854-4c18-ac1b-0a982457c965", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{'health_status': 'Healthy', 'sync_status': 'Synced', 'message': 'successfully synced (all tasks run)', 'source': <ai_core_sdk.models.application_source.ApplicationSource object at 0x7f8dbe7d1270>, 'sync_finished_at': '2024-02-14T03:38:27Z', 'sync_started_at': '2024-02-14T03:38:26Z', 'reconciled_at': '2024-02-14T03:38:28Z', 'sync_ressources_status': [<ai_core_sdk.models.application_resource_sync_status.ApplicationResourceSyncStatus object at 0x7f8dbe7d08b0>]}\n********************************************************************************\n{'name': 'openscale-demo-240213', 'kind': 'ServingTemplate', 'status': 'Synced', 'message': 'servingtemplate.ai.sap.com/openscale-demo-240213 created'}\n"}], "source": "import time\n\ntime.sleep(60)\nresponse = ai_core_client.applications.get_status(application_name = 'openscale-demo-app')\n\nprint(response.__dict__)\nprint('*'*80)\nprint(response.sync_ressources_status[0].__dict__)"}, {"cell_type": "markdown", "id": "822418cb-3cb8-420f-8755-1f57a05e8f80", "metadata": {}, "source": "After your workflows are synced, your\u00a0`Scenario`\u00a0will be automatically created in SAP AI Core. The name and ID of the scenario will be same as the one mentioned in your workflows. After The syncing, your workflow will be recognized as an executable."}, {"cell_type": "markdown", "id": "924f46c1-0b0d-4e12-a6ba-ceecec80368e", "metadata": {}, "source": "<a id=\"create_configuration\"></a>\n### Step 6: Create a deployment configuration"}, {"cell_type": "markdown", "id": "7a2b1eae-59d1-47f5-aba7-f3f8d2e4103e", "metadata": {}, "source": "Here are the important pieces of your configuration:\n\n- The\u00a0`scenario_id`\u00a0should contain the same value as in your executable.\n- The\u00a0`executable_id`\u00a0is the\u00a0name\u00a0key of your executable."}, {"cell_type": "code", "execution_count": 11, "id": "e584ea26-8cd6-4cce-ba7d-20952379764e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{'id': '7b57612b-cb89-46b3-94eb-8ca3e31f2f30', 'message': 'Configuration created'}\n"}], "source": "# Replace the\u00a0artifact_id\u00a0field value with your own ID, then execute the code.\nfrom ai_core_sdk.models import InputArtifactBinding\n\nresponse = ai_core_client.configuration.create(\n    name = \"openscale-demo-conf\",\n    resource_group = \"default\",\n    scenario_id = \"openscale-demo\",\n    executable_id = \"openscale-demo-240213\"\n)\n\nprint(response.__dict__)\nconf_id=response.__dict__['id']"}, {"cell_type": "markdown", "id": "57fa12dc-cdd3-4b79-9ab3-dccd232c474a", "metadata": {}, "source": "<a id=\"start_deployment\"></a>\n### Step 7: Start the deployment"}, {"cell_type": "markdown", "id": "464e59b5-276f-4da9-ad3e-77793b72380f", "metadata": {}, "source": "Execute the code to start the deployment:"}, {"cell_type": "code", "execution_count": 12, "id": "922716d8-5c60-4a09-a595-b34169069018", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{'id': 'd2131b420b9a3efc', 'message': 'Deployment scheduled.', 'deployment_url': '', 'status': <Status.UNKNOWN: 'UNKNOWN'>, 'ttl': None}\n"}], "source": "response = ai_core_client.deployment.create(\n    resource_group = \"default\",\n    configuration_id = conf_id\n)\nprint(response.__dict__)\ndeploy_id=response.__dict__['id']"}, {"cell_type": "markdown", "id": "6b86f5d6-3bfe-4cd1-9935-ea9d24f1b8f2", "metadata": {}, "source": "<div class=\"alert alert-block alert-warning\">\n<b>Important:</b>\n\nNote the unique ID generated of your deployment. You may create multiple deployments using the same configuration ID, each of which will have a unique endpoint.\n\n</div>"}, {"cell_type": "markdown", "id": "8692f3a0-f02a-4c61-9882-32e836fb62b5", "metadata": {}, "source": "Check deployment status:"}, {"cell_type": "code", "execution_count": 13, "id": "0eb49775-c661-41ef-a698-17de06040dab", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Status:  Status.UNKNOWN\n********************************************************************************\nStatus:  Status.UNKNOWN\n********************************************************************************\nStatus:  Status.UNKNOWN\n********************************************************************************\nStatus:  Status.PENDING\n********************************************************************************\nStatus:  Status.PENDING\n********************************************************************************\nStatus:  Status.PENDING\n********************************************************************************\nStatus:  Status.PENDING\n********************************************************************************\nStatus:  Status.PENDING\n********************************************************************************\nStatus:  Status.PENDING\n********************************************************************************\nStatus:  Status.PENDING\n********************************************************************************\nStatus:  Status.RUNNING\n********************************************************************************\n{'id': 'd2131b420b9a3efc', 'configuration_id': '7b57612b-cb89-46b3-94eb-8ca3e31f2f30', 'configuration_name': 'openscale-demo-conf', 'scenario_id': 'openscale-demo', 'status': <Status.RUNNING: 'RUNNING'>, 'target_status': <TargetStatus.RUNNING: 'RUNNING'>, 'created_at': datetime.datetime(2024, 2, 14, 3, 40, 3, tzinfo=datetime.timezone.utc), 'modified_at': datetime.datetime(2024, 2, 14, 3, 45, 17, tzinfo=datetime.timezone.utc), 'status_message': None, 'status_details': None, 'submission_time': datetime.datetime(2024, 2, 14, 3, 41, 1, tzinfo=datetime.timezone.utc), 'start_time': datetime.datetime(2024, 2, 14, 3, 45, 17, tzinfo=datetime.timezone.utc), 'completion_time': None, 'deployment_url': 'https://api.ai.prod.us-east-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d2131b420b9a3efc', 'last_operation': <Operation.CREATE: 'CREATE'>, 'latest_running_configuration_id': '7b57612b-cb89-46b3-94eb-8ca3e31f2f30', 'details': {'resources': {'backend_details': {'predictor': {'resource_plan': 'starter'}}}, 'scaling': {'backend_details': {'predictor': {'max_replicas': '2', 'min_replicas': '1', 'running_replicas': 1}}}}, 'ttl': None}\n"}], "source": "while str(response.status)!='Status.RUNNING':\n\n    response = ai_core_client.deployment.get(\n        resource_group = 'default',\n        deployment_id = deploy_id\n    )\n\n    print(\"Status: \", response.status)\n    print('*'*80)\n    time.sleep(30)\n\nprint(response.__dict__)"}, {"cell_type": "markdown", "id": "09a3d64a-4c56-42d0-a956-7d544724d77f", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n\n<b>Tip:</b>\n\nThis may take a few minutes for the status to change:\u00a0`UNKNOWN`\u00a0>\u00a0`PENDING`\u00a0>\u00a0`RUNNING`.\n\n</div>"}, {"cell_type": "markdown", "id": "7fe658fe", "metadata": {}, "source": "Now query the deployment logs to view its output:"}, {"cell_type": "code", "execution_count": 14, "id": "bf93b5d8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Starting\n---\n[2024-02-14 03:43:58 +0000] [7] [INFO] Starting gunicorn 21.2.0\n---\n[2024-02-14 03:43:58 +0000] [7] [INFO] Listening at: http://0.0.0.0:7000 (7)\n---\n[2024-02-14 03:43:58 +0000] [7] [INFO] Using worker: sync\n---\n[2024-02-14 03:43:58 +0000] [8] [INFO] Booting worker with pid: 8\n---\n"}], "source": "from ai_core_sdk.models import TargetStatus\n\nresponse = ai_core_client.deployment.query_logs(\n    resource_group = \"default\",\n    deployment_id = deploy_id\n)\n\nfor log in response.data.result:\n    print(log.msg)\n    print(\"---\")"}, {"cell_type": "markdown", "id": "143bd57d-b224-4664-a196-0a986e0eef19", "metadata": {}, "source": "<a id=\"make_prediction\"></a>\n### Step 8: Make a prediction"}, {"cell_type": "code", "execution_count": 15, "id": "6a1fbdb6", "metadata": {}, "outputs": [{"data": {"text/plain": "'https://api.ai.prod.us-east-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d2131b420b9a3efc'"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "deployment_url = \"https://api.ai.prod.us-east-1.aws.ml.hana.ondemand.com/v2/inference/deployments/\" + deploy_id\ndeployment_url"}, {"cell_type": "code", "execution_count": 16, "id": "3e759305", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Flask Code: Model is loaded.\n"}], "source": "import requests\n\n# URL\nendpoint = f\"{deployment_url}/v2/greet\" # endpoint implemented in serving engine\nheaders = {\"Authorization\": ai_core_client.rest_client.get_token(),\n           \"AI-Resource-Group\": \"default\"}\nresponse = requests.get(endpoint, headers=headers)\n\nprint(response.text)"}, {"cell_type": "code", "execution_count": 17, "id": "e7d9e2be", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "<Response [200]>\n{'predictions': [{'fields': ['prediction', 'probability'], 'values': [[0, [0.99, 0.01]]]}]}\n"}], "source": "# Prepare a sample input\ntest_input = {'input_data': [{'fields': ['no_of_trainings', 'age', 'previous_year_rating', 'length_of_service', 'kpis_met_above_80_percent', 'any_awards_won', 'avg_training_score', 'department_Finance', 'department_HR', 'department_Legal', 'department_Operations', 'department_Procurement', 'department_R&D', 'department_Sales & Marketing', 'department_Technology', 'education_Below Secondary', \"education_Master's & above\", 'gender_m'],'values': [[1,29,1.0,1,0,0,49,0,0,0,0,0,0,1,0,0,0,0]]}]}\n#records = {'input_data': [{'fields':df_logging.columns.tolist(),'values':df_logging.values.tolist()}]}\n\nendpoint = f\"{deployment_url}/v2/predict\" # endpoint implemented in serving engine\nheaders = {\"Authorization\": ai_core_client.rest_client.get_token(),\n           \"AI-Resource-Group\": \"default\",\n           \"Content-Type\": \"application/json\"}\nresponse = requests.post(endpoint, headers=headers, json=test_input)\n#response = requests.post(endpoint, headers=headers, json=records)\n\nprint(response)\nprint( response.json())"}, {"cell_type": "markdown", "id": "64881a76", "metadata": {}, "source": "<a id=\"predict_and_log\"></a>\n### Step 9: Make a prediction and send logging to OpenScale"}, {"cell_type": "code", "execution_count": 18, "id": "1cfd6e14", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "<Response [200]>\n{'logging_response': 'Payload Logging and Feedback logging Successful', 'model_prediction': {'predictions': [{'fields': ['prediction', 'probability'], 'values': [[0, [0.99, 0.01]]]}]}}\n"}], "source": "endpoint = f\"{deployment_url}/v2/predict_and_log\" # endpoint implemented in serving engine\n\nheaders = {\"Authorization\": ai_core_client.rest_client.get_token(),\n           \"AI-Resource-Group\": \"default\",\n           \"Content-Type\": \"application/json\"}\nresponse = requests.post(endpoint, headers=headers, json=test_input)\n\nprint(response)\nprint( response.json())"}, {"cell_type": "markdown", "id": "6a6de396-80b2-4a2c-978d-29cf7ccd511e", "metadata": {}, "source": "<a id=\"stop_deployment\"></a>\n### Step 10: Stop the deployment"}, {"cell_type": "markdown", "id": "72ce7d86-4ef9-49eb-8feb-dae3a5dfe454", "metadata": {}, "source": "A running deployment incurs cost because it is allocated cloud resources. Stopping the deployment frees up these resources and therefore there is no charge for a deployment of status\u00a0`Stopped`."}, {"cell_type": "code", "execution_count": 19, "id": "91e8f4e7-3d72-4b2f-84de-46f19b3d0830", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{'id': 'd2131b420b9a3efc', 'message': 'Deployment modification scheduled'}\n"}], "source": "from ai_core_sdk.models import TargetStatus\n\nresponse = ai_core_client.deployment.modify(\n    resource_group = \"default\",\n    deployment_id = deploy_id,\n    target_status = TargetStatus.STOPPED\n)\n\nprint(response.__dict__)"}, {"cell_type": "markdown", "id": "14c343df", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n\n<b>Tip:</b>\n\nYou cannot restart a deployment. You must create a new deployment, reusing the configuration. Each deployment will have a different URL.\n</div>"}, {"cell_type": "markdown", "id": "168ab4a8", "metadata": {}, "source": "<a id=\"summary\"></a>\n## Summary"}, {"cell_type": "markdown", "id": "1337858a", "metadata": {}, "source": "Now we've demonstrated how you can deploy an IBM Watson OpenScale Custom ML Engine on SAP AI Core and securely make REST calls to its endpoints to make predictions and then store request and response data to payload and feedback logging tables in IBM Watson OpenScale."}, {"cell_type": "code", "execution_count": null, "id": "b05bcf3c", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3.10", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.13"}}, "nbformat": 4, "nbformat_minor": 5}